{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-olQV4SM_gi"
      },
      "source": [
        "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7_RSdmDM_gj"
      },
      "source": [
        "# Taller: Tokenización de textos  \n",
        "\n",
        "En este taller podrán poner en práctica sus conocimientos sobre preprocesamiento de texto (tokenización). El taller está constituido por 5 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1LUc1URM_gj"
      },
      "source": [
        "## Datos detección de toxicidad en comentarios\n",
        "\n",
        "En este taller se usará el conjunto de datos de detección de toxicidad en comentarios de la base de datos de Kaggle. Cada observación es un comentario que tiene como variable objetivo (target) la probabilidad de ser un comentario tóxico. El objetivo es predecir la toxicidad de cada comentario. Para más detalles pueden visitar el siguiente enlace: [datos](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7isoyKgVM_gk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Udrif31dM_gk"
      },
      "outputs": [],
      "source": [
        "# Importación librerías\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn import metrics\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "ngZD1iqtM_gl",
        "outputId": "2b655e57-461d-4056-e3f9-683809d4704f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                       comment_text    target\n",
              "0  6101457  What are you talking about? What group do Pete...  0.000000\n",
              "1  5342103   NO!, Let him, we need a Conservative government.  0.000000\n",
              "2   743361  Perhaps he took the \"power out of the Cardinal...  0.200000\n",
              "3   551008  As always, yours is dripping with sarcasm, whi...  0.000000\n",
              "4   865998  The dirty little secret is that the price rang...  0.300000\n",
              "5  5790966  Light gets bent by gravity.  However, it isn't...  0.000000\n",
              "6  5241987  This is a bad joke, we rewrite history every t...  0.000000\n",
              "7  6116383  If some homeless actually live peacefully toge...  0.000000\n",
              "8   653122                                   Thanks.  I will.  0.166667\n",
              "9  5439407  Okay.....Should we not be investigating Eric H...  0.500000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4901c5b9-e88d-4138-a261-d5bbd5504330\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6101457</td>\n",
              "      <td>What are you talking about? What group do Pete...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5342103</td>\n",
              "      <td>NO!, Let him, we need a Conservative government.</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>743361</td>\n",
              "      <td>Perhaps he took the \"power out of the Cardinal...</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>551008</td>\n",
              "      <td>As always, yours is dripping with sarcasm, whi...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>865998</td>\n",
              "      <td>The dirty little secret is that the price rang...</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5790966</td>\n",
              "      <td>Light gets bent by gravity.  However, it isn't...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5241987</td>\n",
              "      <td>This is a bad joke, we rewrite history every t...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6116383</td>\n",
              "      <td>If some homeless actually live peacefully toge...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>653122</td>\n",
              "      <td>Thanks.  I will.</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5439407</td>\n",
              "      <td>Okay.....Should we not be investigating Eric H...</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4901c5b9-e88d-4138-a261-d5bbd5504330')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4901c5b9-e88d-4138-a261-d5bbd5504330 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4901c5b9-e88d-4138-a261-d5bbd5504330');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Carga de datos de archivos .csv\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/Toxicity.zip')\n",
        "df = df[['id','comment_text', 'target']]\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.iloc[:20000,:]"
      ],
      "metadata": {
        "id": "W6Q9WX9tTBB7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "DGYdOhpRM_gl",
        "outputId": "d5d1340f-65df-4a2b-9303-7de855ac31aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Impresión histograma de variable de interés (y)\n",
        "df.target.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tExaEzMM_gl",
        "outputId": "84de5e37-742f-4749-858f-7d7d1d16be64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    20000.000000\n",
              "mean         0.102851\n",
              "std          0.198180\n",
              "min          0.000000\n",
              "25%          0.000000\n",
              "50%          0.000000\n",
              "75%          0.166667\n",
              "max          1.000000\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Separación de variable de interés (y)\n",
        "y = df.target\n",
        "y.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fUpnfrqrM_gm"
      },
      "outputs": [],
      "source": [
        "# Separación de variables predictoras (X), solo se considera el texto de la noticia\n",
        "X = df.comment_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XxZk-2AsM_gm"
      },
      "outputs": [],
      "source": [
        "# Separación de datos en set de entrenamiento y test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs3tDAPkM_gm"
      },
      "source": [
        "## Punto 1 - Tokenización con CountVectorizer\n",
        "\n",
        "En la celda 1 creen y entrenen el modelo de regresión de su preferencia, para que prediga la probabilidad de que un comentario sea tóxico, usando los set de entrenamiento y test definidos anteriormente. Usen la función **CountVectorizer** para preprocesar los comentarios y presenten el desempeño del modelo con la métrica del MSE.\n",
        "\n",
        "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0QZBcX76M_gm"
      },
      "outputs": [],
      "source": [
        "# Creación de matrices de documentos usando CountVectorizer a partir de X\n",
        "vect_V = CountVectorizer()\n",
        "X_dtm_V = vect_V.fit_transform(X_train)\n",
        "temp_A=X_dtm_V.todense()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = vect_V.transform(X_train)\n",
        "print(V.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTv0gEqRXBhN",
        "outputId": "16e50022-9b98-46ba-9b83-7b823bbf6e0f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el objeto de Regresión Linear\n",
        "regrV = LinearRegression()"
      ],
      "metadata": {
        "id": "6iBec_TrgsFI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos nuestro modelo\n",
        "regrV.fit(V, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1efuWRiKi35u",
        "outputId": "49b9683f-009e-4010-ffa6-79ffc4b235db"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos las predicciones que en definitiva una línea (en este caso, al ser 2D)\n",
        "yPredV = regrV.predict(V)"
      ],
      "metadata": {
        "id": "O3sp-s7tjLEx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos los coeficienetes obtenidos, En nuestro caso, serán la Tangente\n",
        "print('Coefficients: \\n', regrV.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swAT0pgSjSiq",
        "outputId": "903d30c5-f531-4707-cc83-f44c31c346c5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: \n",
            " [ 0.06183978  0.05930974  0.16194883 ...  0.00177132 -0.01746789\n",
            " -0.01746789]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Este es el valor donde corta el eje Y (en X=0)\n",
        "print('Independent term: \\n', regrV.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ACRzDJmjqA_",
        "outputId": "d80c5e33-e484-4e77-8e63-cc6cad44ec80"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Independent term: \n",
            " 0.019238195572316333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error Cuadrado Medio\n",
        "CountVectorizer = metrics.mean_squared_error(y_train, yPredV)\n",
        "print(\"Mean squared error:\" , CountVectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbg4w6JAjYS2",
        "outputId": "65bfc03d-8855-439b-edaf-e11096c07a54"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 8.396092617504346e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0VqWqGMM_gm"
      },
      "source": [
        "## Punto 2 - Tokenización con CountVectorizer y trigramas\n",
        "\n",
        "En la celda 2 creen y entrenen el mismo modelo de regresión del punto anterior (es decir si usaron un RandomForestRegresor usen nuevamente ese regresor), para que prediga la probabilidad de que un comentario sea tóxico, usando los set de entrenamiento y test definidos anteriormente. Usen la función CountVectorizer **considerando trigramas** para preprocesar los comentarios y presenten el desempeño del modelo con la métrica del MSE.\n",
        "\n",
        "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NdRdfjwuM_gn"
      },
      "outputs": [],
      "source": [
        "# Creación de matrices de documentos usando CountVectorizer a partir de X\n",
        "vect_T = CountVectorizer(ngram_range = (3, 3))\n",
        "X_dtm_T = vect_T.fit_transform(X_train)\n",
        "temp_T=X_dtm_T.todense()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de diccionario de palabras con su respectivo ID asignado\n",
        "vect_T.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvu4ngayqMwn",
        "outputId": "cdf79f3d-811c-42f8-e916-16c1967012aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'so much for': 393887,\n",
              " 'much for state': 280954,\n",
              " 'for state sovereignty': 158603,\n",
              " 'state sovereignty and': 402238,\n",
              " 'sovereignty and the': 398553,\n",
              " 'and the righting': 35464,\n",
              " 'the righting the': 444338,\n",
              " 'righting the people': 372393,\n",
              " 'the people to': 441393,\n",
              " 'people to self': 337863,\n",
              " 'to self rule': 479777,\n",
              " 'self rule this': 382755,\n",
              " 'rule this business': 374319,\n",
              " 'this business of': 462625,\n",
              " 'business of enacting': 77099,\n",
              " 'of enacting social': 302606,\n",
              " 'enacting social change': 136180,\n",
              " 'social change by': 394838,\n",
              " 'change by the': 90869,\n",
              " 'by the courts': 81757,\n",
              " 'the courts needs': 429960,\n",
              " 'courts needs to': 109283,\n",
              " 'needs to end': 286546,\n",
              " 'to end let': 473587,\n",
              " 'end let the': 136338,\n",
              " 'let the people': 251842,\n",
              " 'the people decide': 441315,\n",
              " 'people decide makes': 336865,\n",
              " 'decide makes no': 115091,\n",
              " 'makes no sense': 263912,\n",
              " 'no sense to': 290960,\n",
              " 'sense to call': 383540,\n",
              " 'to call ourselves': 471851,\n",
              " 'call ourselves democracy': 82784,\n",
              " 'ourselves democracy when': 328272,\n",
              " 'democracy when people': 116963,\n",
              " 'when people get': 518261,\n",
              " 'people get to': 337061,\n",
              " 'get to decide': 169782,\n",
              " 'to decide all': 472784,\n",
              " 'no thanks you': 291079,\n",
              " 'thanks you whip': 417548,\n",
              " 'you whip your': 544902,\n",
              " 'whip your own': 521005,\n",
              " 'your own cream': 546660,\n",
              " 'own cream oh': 331131,\n",
              " 'cream oh wait': 109833,\n",
              " 'oh wait you': 313031,\n",
              " 'wait you mean': 504317,\n",
              " 'you mean whipped': 543065,\n",
              " 'mean whipped cream': 269699,\n",
              " 'whipped cream gosh': 521006,\n",
              " 'cream gosh guess': 109831,\n",
              " 'gosh guess you': 174557,\n",
              " 'guess you flunk': 178288,\n",
              " 'you flunk as': 542002,\n",
              " 'flunk as barista': 153950,\n",
              " 'as barista too': 48693,\n",
              " 'barista too ah': 58299,\n",
              " 'too ah well': 484039,\n",
              " 'most mountain bikers': 279085,\n",
              " 'mountain bikers know': 279676,\n",
              " 'bikers know ride': 70162,\n",
              " 'know ride in': 244931,\n",
              " 'ride in groups': 371387,\n",
              " 'in groups carry': 211084,\n",
              " 'groups carry bear': 177697,\n",
              " 'carry bear spray': 88228,\n",
              " 'bear spray and': 62991,\n",
              " 'spray and make': 400284,\n",
              " 'and make noise': 30747,\n",
              " 'make noise that': 263403,\n",
              " 'noise that the': 291458,\n",
              " 'that the reason': 423396,\n",
              " 'the reason why': 443635,\n",
              " 'reason why more': 362962,\n",
              " 'why more cyclists': 524044,\n",
              " 'more cyclists aren': 277186,\n",
              " 'cyclists aren mauled': 112492,\n",
              " 'aren mauled direct': 46954,\n",
              " 'mauled direct quote': 267627,\n",
              " 'direct quote from': 121556,\n",
              " 'quote from the': 357885,\n",
              " 'from the author': 164726,\n",
              " 'the author so': 426622,\n",
              " 'author so why': 55383,\n",
              " 'so why aren': 394633,\n",
              " 'why aren more': 523760,\n",
              " 'aren more cyclists': 46955,\n",
              " 'more cyclists mauled': 277187,\n",
              " 'cyclists mauled there': 112496,\n",
              " 'mauled there no': 267629,\n",
              " 'there no easy': 455696,\n",
              " 'no easy way': 289959,\n",
              " 'easy way to': 132494,\n",
              " 'way to figure': 510281,\n",
              " 'to figure this': 474037,\n",
              " 'figure this out': 151159,\n",
              " 'this out sounds': 464327,\n",
              " 'out sounds like': 329076,\n",
              " 'sounds like the': 398208,\n",
              " 'like the author': 255106,\n",
              " 'the author has': 426604,\n",
              " 'author has an': 55344,\n",
              " 'has an agenda': 182197,\n",
              " 'an agenda against': 22412,\n",
              " 'agenda against mountain': 13629,\n",
              " 'against mountain bikers': 13227,\n",
              " 'you are making': 540770,\n",
              " 'are making stuff': 44292,\n",
              " 'making stuff up': 264214,\n",
              " 'stuff up please': 406484,\n",
              " 'up please go': 495992,\n",
              " 'please go comment': 342890,\n",
              " 'go comment somewhere': 171863,\n",
              " 'comment somewhere else': 99375,\n",
              " 'somewhere else what': 397553,\n",
              " 'else what has': 135252,\n",
              " 'what has been': 516224,\n",
              " 'has been accomplished': 182303,\n",
              " 'been accomplished with': 64326,\n",
              " 'accomplished with this': 8276,\n",
              " 'with this virtue': 530903,\n",
              " 'this virtue signaling': 465242,\n",
              " 'virtue signaling protest': 502622,\n",
              " 'signaling protest nothing': 389665,\n",
              " 'the nice thing': 439689,\n",
              " 'nice thing about': 289290,\n",
              " 'thing about actual': 460367,\n",
              " 'about actual crime': 5461,\n",
              " 'actual crime reports': 9624,\n",
              " 'crime reports is': 110461,\n",
              " 'reports is that': 367728,\n",
              " 'is that some': 229311,\n",
              " 'that some of': 422719,\n",
              " 'some of the': 396236,\n",
              " 'of the stories': 309318,\n",
              " 'the stories are': 446503,\n",
              " 'stories are far': 404927,\n",
              " 'are far more': 43455,\n",
              " 'far more fantastic': 148260,\n",
              " 'more fantastic anything': 277328,\n",
              " 'fantastic anything fiction': 148067,\n",
              " 'anything fiction writer': 40669,\n",
              " 'fiction writer would': 150854,\n",
              " 'writer would make': 537166,\n",
              " 'would make up': 536183,\n",
              " 'my dad could': 282716,\n",
              " 'dad could retire': 112576,\n",
              " 'could retire now': 107431,\n",
              " 'retire now but': 370290,\n",
              " 'now but he': 297299,\n",
              " 'but he doesn': 78044,\n",
              " 'he doesn want': 188954,\n",
              " 'doesn want to': 126351,\n",
              " 'want to and': 505015,\n",
              " 'to and keeps': 470221,\n",
              " 'and keeps trying': 30178,\n",
              " 'keeps trying to': 242915,\n",
              " 'trying to get': 490763,\n",
              " 'to get job': 474520,\n",
              " 'get job he': 169241,\n",
              " 'job he would': 238992,\n",
              " 'he would love': 190713,\n",
              " 'would love to': 536165,\n",
              " 'love to still': 260969,\n",
              " 'to still be': 480456,\n",
              " 'still be working': 403714,\n",
              " 'be working if': 62878,\n",
              " 'working if based': 534010,\n",
              " 'if based on': 205028,\n",
              " 'based on this': 58647,\n",
              " 'on this he': 317020,\n",
              " 'this he can': 463349,\n",
              " 'he can well': 188674,\n",
              " 'can well afford': 85360,\n",
              " 'well afford to': 513501,\n",
              " 'afford to retire': 11802,\n",
              " 'to retire why': 479275,\n",
              " 'retire why doesn': 370293,\n",
              " 'why doesn he': 523869,\n",
              " 'doesn he do': 126118,\n",
              " 'he do just': 188897,\n",
              " 'do just that': 124102,\n",
              " 'just that and': 241712,\n",
              " 'that and devote': 417875,\n",
              " 'and devote his': 26786,\n",
              " 'devote his time': 119354,\n",
              " 'his time to': 198049,\n",
              " 'time to volunteer': 469004,\n",
              " 'to volunteer work': 482752,\n",
              " 'volunteer work plenty': 503038,\n",
              " 'work plenty of': 533531,\n",
              " 'plenty of charities': 343104,\n",
              " 'of charities out': 301454,\n",
              " 'charities out there': 91715,\n",
              " 'out there and': 329259,\n",
              " 'there and they': 455004,\n",
              " 'and they re': 35962,\n",
              " 'they re all': 459304,\n",
              " 're all eager': 360141,\n",
              " 'all eager for': 16309,\n",
              " 'eager for workers': 131826,\n",
              " 'good to know': 174217,\n",
              " 'to know it': 476283,\n",
              " 'know it strengthens': 244830,\n",
              " 'it strengthens my': 235896,\n",
              " 'strengthens my respect': 405622,\n",
              " 'my respect for': 283493,\n",
              " 'respect for potus': 369369,\n",
              " 'you use seats': 544661,\n",
              " 'use seats all': 498425,\n",
              " 'seats all the': 380454,\n",
              " 'all the time': 17593,\n",
              " 'the time in': 447491,\n",
              " 'time in your': 468557,\n",
              " 'in your honda': 215748,\n",
              " 'your honda civic': 546194,\n",
              " 'honda civic guess': 199769,\n",
              " 'civic guess you': 94873,\n",
              " 'guess you re': 178298,\n",
              " 'you re just': 543667,\n",
              " 're just hauling': 360427,\n",
              " 'just hauling less': 241000,\n",
              " 'hauling less air': 184282,\n",
              " 'ah different strokes': 14494,\n",
              " 'different strokes sil': 121115,\n",
              " 'strokes sil the': 405829,\n",
              " 'sil the elephant': 389856,\n",
              " 'the elephant in': 431725,\n",
              " 'elephant in my': 134900,\n",
              " 'in my room': 212193,\n",
              " 'my room is': 283505,\n",
              " 'room is shelter': 373762,\n",
              " 'is shelter anyhow': 228596,\n",
              " 'shelter anyhow not': 386649,\n",
              " 'anyhow not 100': 40173,\n",
              " 'not 100 er': 292377,\n",
              " '100 er indeed': 703,\n",
              " 'er indeed progess': 138602,\n",
              " 'indeed progess is': 217217,\n",
              " 'progess is being': 352507,\n",
              " 'is being made': 223617,\n",
              " 'he probably does': 189925,\n",
              " 'probably does have': 351045,\n",
              " 'does have crimes': 125461,\n",
              " 'have crimes leading': 185178,\n",
              " 'crimes leading back': 110549,\n",
              " 'leading back years': 249059,\n",
              " 'back years related': 56870,\n",
              " 'years related to': 539349,\n",
              " 'related to his': 365594,\n",
              " 'to his shady': 475507,\n",
              " 'his shady lobbying': 197853,\n",
              " 'shady lobbying work': 385393,\n",
              " 'lobbying work there': 258263,\n",
              " 'work there is': 533590,\n",
              " 'there is still': 455566,\n",
              " 'is still no': 228974,\n",
              " 'still no evidence': 403972,\n",
              " 'no evidence that': 290000,\n",
              " 'evidence that all': 142536,\n",
              " 'that all of': 417740,\n",
              " 'all of the': 16893,\n",
              " 'of the charges': 308233,\n",
              " 'the charges are': 428465,\n",
              " 'charges are unrelated': 91650,\n",
              " 'are unrelated to': 46312,\n",
              " 'unrelated to the': 494805,\n",
              " 'to the 2016': 481026,\n",
              " 'the 2016 campaign': 425221,\n",
              " '2016 campaign also': 2508,\n",
              " 'campaign also revealed': 83455,\n",
              " 'also revealed in': 19738,\n",
              " 'revealed in this': 370615,\n",
              " 'in this unsealing': 215113,\n",
              " 'this unsealing is': 465203,\n",
              " 'unsealing is that': 494842,\n",
              " 'is that an': 229194,\n",
              " 'that an adviser': 417849,\n",
              " 'an adviser to': 22384,\n",
              " 'adviser to trump': 11458,\n",
              " 'to trump already': 482411,\n",
              " 'trump already plead': 488660,\n",
              " 'already plead guilty': 19157,\n",
              " 'plead guilty to': 342787,\n",
              " 'guilty to lying': 178463,\n",
              " 'to lying to': 476696,\n",
              " 'lying to the': 261731,\n",
              " 'to the fbi': 481306,\n",
              " 'the fbi about': 432604,\n",
              " 'fbi about his': 149054,\n",
              " 'about his contacts': 5983,\n",
              " 'his contacts with': 196744,\n",
              " 'contacts with russians': 104340,\n",
              " 'with russians during': 529828,\n",
              " 'russians during the': 375231,\n",
              " 'during the campaign': 131379,\n",
              " 'the campaign the': 428030,\n",
              " 'campaign the odds': 83559,\n",
              " 'the odds are': 440008,\n",
              " 'odds are looking': 299994,\n",
              " 'are looking little': 44241,\n",
              " 'looking little worse': 259709,\n",
              " 'little worse for': 257160,\n",
              " 'worse for folks': 534884,\n",
              " 'for folks who': 156356,\n",
              " 'folks who want': 154362,\n",
              " 'who want to': 523125,\n",
              " 'want to argue': 505017,\n",
              " 'to argue that': 470372,\n",
              " 'argue that this': 47096,\n",
              " 'that this is': 423771,\n",
              " 'this is all': 463508,\n",
              " 'is all related': 223009,\n",
              " 'all related to': 17108,\n",
              " 'related to pre': 365598,\n",
              " 'to pre trump': 478315,\n",
              " 'pre trump dealings': 348005,\n",
              " 'trump dealings by': 488887,\n",
              " 'dealings by manafort': 114328,\n",
              " 'there is no': 455512,\n",
              " 'is no parallel': 226857,\n",
              " 'no parallel between': 290697,\n",
              " 'parallel between the': 332694,\n",
              " 'between the two': 69298,\n",
              " 'the two harper': 448156,\n",
              " 'two harper decision': 491686,\n",
              " 'harper decision made': 181937,\n",
              " 'decision made the': 115272,\n",
              " 'made the census': 262243,\n",
              " 'the census voluntary': 428356,\n",
              " 'census voluntary and': 89910,\n",
              " 'voluntary and trudeau': 503015,\n",
              " 'and trudeau paid': 36391,\n",
              " 'trudeau paid off': 488130,\n",
              " 'paid off convicted': 332147,\n",
              " 'off convicted terrorist': 311626,\n",
              " 'convicted terrorist and': 105550,\n",
              " 'terrorist and murderer': 415801,\n",
              " 'and murderer who': 31264,\n",
              " 'murderer who fought': 281733,\n",
              " 'who fought against': 522064,\n",
              " 'fought against canadian': 161747,\n",
              " 'against canadian soldiers': 13085,\n",
              " 'canadian soldiers and': 86231,\n",
              " 'soldiers and our': 395478,\n",
              " 'and our allies': 32181,\n",
              " 'our allies with': 326847,\n",
              " 'allies with taliban': 18194,\n",
              " 'with taliban khadr': 530063,\n",
              " 'taliban khadr is': 412443,\n",
              " 'khadr is not': 243209,\n",
              " 'is not canadian': 226993,\n",
              " 'not canadian he': 293116,\n",
              " 'canadian he is': 86129,\n",
              " 'he is traitor': 189518,\n",
              " 'is traitor who': 230108,\n",
              " 'traitor who spent': 486379,\n",
              " 'who spent total': 522884,\n",
              " 'spent total of': 399810,\n",
              " 'total of 15': 485194,\n",
              " 'of 15 months': 300062,\n",
              " '15 months in': 1292,\n",
              " 'months in canada': 276618,\n",
              " 'in canada prior': 209877,\n",
              " 'canada prior to': 85821,\n",
              " 'prior to being': 350500,\n",
              " 'to being captured': 471459,\n",
              " 'being captured in': 66456,\n",
              " 'captured in afghanistan': 87228,\n",
              " 'in afghanistan fighting': 209100,\n",
              " 'afghanistan fighting for': 11883,\n",
              " 'fighting for the': 151094,\n",
              " 'for the taliban': 159420,\n",
              " 'the taliban the': 447006,\n",
              " 'taliban the decision': 412445,\n",
              " 'the decision to': 430536,\n",
              " 'decision to pay': 115308,\n",
              " 'to pay convicted': 477970,\n",
              " 'pay convicted taliban': 335220,\n",
              " 'convicted taliban terrorist': 105549,\n",
              " 'taliban terrorist and': 412444,\n",
              " 'terrorist and traitor': 415802,\n",
              " 'and traitor is': 36324,\n",
              " 'traitor is grounded': 486373,\n",
              " 'is grounded in': 225270,\n",
              " 'grounded in contemptible': 177458,\n",
              " 'in contemptible moral': 210207,\n",
              " 'contemptible moral cowardice': 104424,\n",
              " 'moral cowardice khadr': 276806,\n",
              " 'cowardice khadr is': 109509,\n",
              " 'khadr is owed': 243210,\n",
              " 'is owed nothing': 227680,\n",
              " 'owed nothing khadr': 330989,\n",
              " 'nothing khadr was': 296793,\n",
              " 'khadr was filmed': 243230,\n",
              " 'was filmed building': 506887,\n",
              " 'filmed building the': 151363,\n",
              " 'building the same': 76200,\n",
              " 'the same type': 445071,\n",
              " 'same type of': 377209,\n",
              " 'type of ied': 492017,\n",
              " 'of ied that': 303996,\n",
              " 'ied that killed': 204942,\n",
              " 'that killed canadian': 420769,\n",
              " 'killed canadian soldiers': 243724,\n",
              " 'canadian soldiers paying': 86234,\n",
              " 'soldiers paying off': 395495,\n",
              " 'paying off khadr': 335748,\n",
              " 'off khadr is': 311722,\n",
              " 'khadr is mark': 243208,\n",
              " 'is mark of': 226407,\n",
              " 'mark of international': 266368,\n",
              " 'of international shame': 304243,\n",
              " 'international shame the': 220738,\n",
              " 'shame the decision': 385515,\n",
              " 'the decision is': 430530,\n",
              " 'decision is inexcusable': 115258,\n",
              " 'is inexcusable and': 225760,\n",
              " 'inexcusable and unforgivable': 218026,\n",
              " 'and unforgivable khadr': 36563,\n",
              " 'unforgivable khadr continued': 493781,\n",
              " 'khadr continued presence': 243199,\n",
              " 'continued presence in': 104715,\n",
              " 'presence in canada': 348527,\n",
              " 'in canada is': 209861,\n",
              " 'canada is stain': 85723,\n",
              " 'is stain on': 228911,\n",
              " 'stain on the': 400753,\n",
              " 'on the country': 316361,\n",
              " 'it is the': 234379,\n",
              " 'is the important': 229524,\n",
              " 'the important part': 435366,\n",
              " 'important part to': 208361,\n",
              " 'part to me': 333444,\n",
              " 'to me we': 477022,\n",
              " 'me we already': 269359,\n",
              " 'we already know': 510583,\n",
              " 'already know the': 19111,\n",
              " 'know the guy': 245066,\n",
              " 'the guy going': 434448,\n",
              " 'guy going to': 178876,\n",
              " 'going to be': 173026,\n",
              " 'to be okay': 471067,\n",
              " 'think would have': 461867,\n",
              " 'would have noticed': 535978,\n",
              " 'have noticed the': 186385,\n",
              " 'noticed the dead': 297056,\n",
              " 'the dead mouse': 430441,\n",
              " 'dead mouse before': 114046,\n",
              " 'mouse before biting': 279714,\n",
              " 'before biting down': 65483,\n",
              " 'biting down too': 71180,\n",
              " 'down too always': 129536,\n",
              " 'too always look': 484041,\n",
              " 'always look at': 20395,\n",
              " 'look at restaurant': 259362,\n",
              " 'at restaurant food': 53612,\n",
              " 'restaurant food with': 369908,\n",
              " 'food with some': 154746,\n",
              " 'with some degree': 529942,\n",
              " 'some degree of': 395926,\n",
              " 'degree of skepticism': 116388,\n",
              " 'of skepticism also': 307376,\n",
              " 'skepticism also think': 391673,\n",
              " 'also think that': 19857,\n",
              " 'think that would': 461666,\n",
              " 'that would have': 424798,\n",
              " 'would have chewed': 535913,\n",
              " 'have chewed submarine': 185059,\n",
              " 'chewed submarine sandwich': 92312,\n",
              " 'submarine sandwich before': 406849,\n",
              " 'sandwich before wolfing': 377421,\n",
              " 'before wolfing it': 65861,\n",
              " 'wolfing it down': 531874,\n",
              " 'it down and': 233251,\n",
              " 'down and that': 129151,\n",
              " 'and that would': 34913,\n",
              " 'would have detected': 535920,\n",
              " 'have detected two': 185211,\n",
              " 'detected two inch': 118943,\n",
              " 'two inch piece': 491705,\n",
              " 'inch piece of': 215980,\n",
              " 'piece of corrugated': 340927,\n",
              " 'of corrugated plastic': 301867,\n",
              " 'corrugated plastic before': 106361,\n",
              " 'plastic before swallowing': 342285,\n",
              " 'before swallowing it': 65703,\n",
              " 'swallowing it but': 410468,\n",
              " 'it but maybe': 232795,\n",
              " 'but maybe the': 78442,\n",
              " 'maybe the lady': 268285,\n",
              " 'the lady was': 436324,\n",
              " 'lady was distracted': 246291,\n",
              " 'was distracted from': 506741,\n",
              " 'distracted from her': 123139,\n",
              " 'from her chewing': 163930,\n",
              " 'her chewing by': 192880,\n",
              " 'chewing by her': 92314,\n",
              " 'by her driving': 80780,\n",
              " 'her driving don': 192955,\n",
              " 'driving don swallow': 130380,\n",
              " 'don swallow buffalo': 127948,\n",
              " 'swallow buffalo wings': 410460,\n",
              " 'buffalo wings without': 75939,\n",
              " 'wings without locating': 527261,\n",
              " 'without locating the': 531590,\n",
              " 'locating the little': 258505,\n",
              " 'the little bones': 437287,\n",
              " 'little bones and': 256865,\n",
              " 'bones and spitting': 72794,\n",
              " 'and spitting them': 34301,\n",
              " 'spitting them out': 400027,\n",
              " 'them out and': 453274,\n",
              " 'out and learned': 328359,\n",
              " 'and learned early': 30335,\n",
              " 'learned early to': 249342,\n",
              " 'early to use': 131970,\n",
              " 'to use my': 482644,\n",
              " 'use my mouth': 498349,\n",
              " 'my mouth to': 283187,\n",
              " 'mouth to separate': 279754,\n",
              " 'to separate tiny': 479843,\n",
              " 'separate tiny fish': 383801,\n",
              " 'tiny fish bones': 469401,\n",
              " 'fish bones from': 153047,\n",
              " 'bones from the': 72796,\n",
              " 'from the good': 164858,\n",
              " 'the good part': 433875,\n",
              " 'good part perhaps': 173999,\n",
              " 'part perhaps subway': 333406,\n",
              " 'perhaps subway should': 338874,\n",
              " 'subway should sue': 407151,\n",
              " 'should sue ms': 388354,\n",
              " 'sue ms juckert': 407815,\n",
              " 'ms juckert for': 280579,\n",
              " 'juckert for misdemeanor': 239907,\n",
              " 'for misdemeanor chewing': 157374,\n",
              " 'the provinces would': 443061,\n",
              " 'provinces would like': 354881,\n",
              " 'would like all': 536124,\n",
              " 'like all of': 254113,\n",
              " 'all of us': 16900,\n",
              " 'of us to': 310724,\n",
              " 'us to forget': 497894,\n",
              " 'to forget that': 474275,\n",
              " 'forget that the': 161143,\n",
              " 'that the federal': 423164,\n",
              " 'the federal government': 432680,\n",
              " 'federal government also': 149419,\n",
              " 'government also has': 175150,\n",
              " 'also has mandate': 19484,\n",
              " 'has mandate for': 183143,\n",
              " 'mandate for all': 264983,\n",
              " 'for all of': 155151,\n",
              " 'all of our': 16885,\n",
              " 'of our health': 306014,\n",
              " 'our health it': 327435,\n",
              " 'health it isn': 191214,\n",
              " 'it isn just': 234466,\n",
              " 'isn just up': 231302,\n",
              " 'just up to': 241860,\n",
              " 'up to the': 496374,\n",
              " 'to the provinces': 481681,\n",
              " 'the provinces how': 443054,\n",
              " 'provinces how money': 354860,\n",
              " 'how money is': 201932,\n",
              " 'money is spent': 275973,\n",
              " 'is spent quebec': 228894,\n",
              " 'spent quebec walking': 399797,\n",
              " 'quebec walking out': 357147,\n",
              " 'walking out should': 504581,\n",
              " 'out should give': 329052,\n",
              " 'should give you': 387933,\n",
              " 'give you an': 170931,\n",
              " 'you an indication': 540544,\n",
              " 'an indication that': 23382,\n",
              " 'indication that trudeau': 217461,\n",
              " 'that trudeau has': 423942,\n",
              " 'trudeau has hit': 488048,\n",
              " 'has hit nerve': 182969,\n",
              " 'hit nerve here': 198566,\n",
              " 'nerve here the': 287036,\n",
              " 'here the provinces': 193994,\n",
              " 'the provinces want': 443060,\n",
              " 'provinces want to': 354878,\n",
              " 'want to keep': 505120,\n",
              " 'to keep wasting': 476206,\n",
              " 'keep wasting more': 242800,\n",
              " 'wasting more money': 509268,\n",
              " 'more money on': 277704,\n",
              " 'money on over': 276043,\n",
              " 'on over bloated': 315655,\n",
              " 'over bloated management': 330058,\n",
              " 'bloated management out': 71881,\n",
              " 'management out of': 264894,\n",
              " 'out of control': 328822,\n",
              " 'of control physician': 301831,\n",
              " 'control physician billing': 105236,\n",
              " 'physician billing and': 340629,\n",
              " 'billing and doing': 70428,\n",
              " 'and doing nothing': 27001,\n",
              " 'doing nothing on': 126690,\n",
              " 'nothing on mental': 296841,\n",
              " 'on mental health': 315400,\n",
              " 'mental health or': 271515,\n",
              " 'health or real': 191230,\n",
              " 'or real innovation': 323591,\n",
              " 'real innovation in': 361667,\n",
              " 'innovation in home': 218912,\n",
              " 'in home care': 211374,\n",
              " 'well it goes': 513836,\n",
              " 'it goes back': 233572,\n",
              " 'goes back few': 172726,\n",
              " 'back few decades': 56499,\n",
              " 'few decades and': 150552,\n",
              " 'decades and remember': 114904,\n",
              " 'and remember the': 33275,\n",
              " 'remember the acorn': 366534,\n",
              " 'the acorn doesn': 425551,\n",
              " 'acorn doesn fall': 8851,\n",
              " 'doesn fall far': 126072,\n",
              " 'fall far from': 147294,\n",
              " 'far from the': 148199,\n",
              " 'from the tree': 165104,\n",
              " 'the tree for': 447849,\n",
              " 'tree for your': 487125,\n",
              " 'for your entertainment': 160441,\n",
              " 'your entertainment sit': 545876,\n",
              " 'entertainment sit back': 137832,\n",
              " 'sit back and': 391209,\n",
              " 'back and enjoy': 56432,\n",
              " 'and enjoy the': 27364,\n",
              " 'enjoy the show': 137235,\n",
              " 'the show https': 445634,\n",
              " 'show https www': 388645,\n",
              " 'https www youtube': 203124,\n",
              " 'www youtube com': 537965,\n",
              " 'youtube com watch': 547684,\n",
              " 'com watch detsqq22uwc': 98373,\n",
              " 'am completely appalled': 20606,\n",
              " 'completely appalled by': 101410,\n",
              " 'appalled by perlow': 41246,\n",
              " 'by perlow rubber': 81330,\n",
              " 'perlow rubber stamping': 339029,\n",
              " 'rubber stamping this': 374174,\n",
              " 'stamping this case': 400814,\n",
              " 'this case as': 462664,\n",
              " 'case as justified': 88441,\n",
              " 'as justified are': 49663,\n",
              " 'justified are you': 242103,\n",
              " 'are you kidding': 46614,\n",
              " 'you kidding me': 542688,\n",
              " 'kidding me your': 243393,\n",
              " 'me your joke': 269438,\n",
              " 'your joke as': 546281,\n",
              " 'joke as prosecutor': 239639,\n",
              " 'as prosecutor and': 50217,\n",
              " 'prosecutor and should': 353863,\n",
              " 'and should have': 33953,\n",
              " 'should have your': 388042,\n",
              " 'have your license': 187790,\n",
              " 'your license to': 546366,\n",
              " 'license to practice': 253236,\n",
              " 'to practice law': 478301,\n",
              " 'practice law taken': 347777,\n",
              " 'law taken for': 248169,\n",
              " 'taken for misconduct': 412025,\n",
              " 'for misconduct if': 157373,\n",
              " 'misconduct if you': 274612,\n",
              " 'if you dont': 206481,\n",
              " 'you dont lose': 541827,\n",
              " 'dont lose sleep': 128725,\n",
              " 'lose sleep over': 259989,\n",
              " 'sleep over this': 392061,\n",
              " 'over this your': 330526,\n",
              " 'this your about': 465558,\n",
              " 'your about as': 545340,\n",
              " 'about as reptilian': 5560,\n",
              " 'as reptilian and': 50300,\n",
              " 'reptilian and cold': 367991,\n",
              " 'and cold blooded': 26068,\n",
              " 'cold blooded as': 97565,\n",
              " 'blooded as any': 72053,\n",
              " 'as any hypocrite': 48631,\n",
              " 'any hypocrite and': 39517,\n",
              " 'hypocrite and corrupt': 204152,\n",
              " 'and corrupt prosecutor': 26344,\n",
              " 'corrupt prosecutor if': 106409,\n",
              " 'prosecutor if ever': 353866,\n",
              " 'if ever seen1': 205165,\n",
              " 'ever seen1 sure': 141262,\n",
              " 'seen1 sure she': 382467,\n",
              " 'sure she is': 409682,\n",
              " 'she is real': 386143,\n",
              " 'is real proud': 228174,\n",
              " 'real proud of': 361789,\n",
              " 'proud of herself': 354305,\n",
              " 'of herself for': 303558,\n",
              " 'herself for just': 194233,\n",
              " 'for just letting': 157031,\n",
              " 'just letting police': 241132,\n",
              " 'letting police off': 252123,\n",
              " 'police off on': 344092,\n",
              " 'off on murder': 311771,\n",
              " 'on murder of': 315447,\n",
              " 'murder of fellow': 281692,\n",
              " 'of fellow citizen': 302907,\n",
              " 'fellow citizen had': 150119,\n",
              " 'citizen had this': 94375,\n",
              " 'had this situation': 180092,\n",
              " 'this situation had': 464817,\n",
              " 'situation had non': 391432,\n",
              " 'had non officer': 179823,\n",
              " 'non officer be': 291640,\n",
              " 'officer be the': 312457,\n",
              " 'be the shooter': 62440,\n",
              " 'the shooter guarantee': 445597,\n",
              " 'shooter guarantee lane': 387005,\n",
              " 'guarantee lane county': 178041,\n",
              " 'lane county district': 246657,\n",
              " 'county district attorney': 108579,\n",
              " 'district attorney office': 123194,\n",
              " 'attorney office corrupt': 55063,\n",
              " 'office corrupt would': 312321,\n",
              " 'corrupt would have': 106425,\n",
              " 'would have slapped': 536002,\n",
              " 'have slapped several': 186908,\n",
              " 'slapped several different': 391930,\n",
              " 'several different variations': 384972,\n",
              " 'different variations on': 121160,\n",
              " 'variations on them': 499937,\n",
              " 'on them just': 316933,\n",
              " 'them just so': 453111,\n",
              " 'just so you': 241641,\n",
              " 'so you can': 394721,\n",
              " 'you can be': 541184,\n",
              " 'can be sure': 83955,\n",
              " 'be sure one': 62221,\n",
              " 'sure one sticks': 409665,\n",
              " 'one sticks quit': 318798,\n",
              " 'sticks quit having': 403636,\n",
              " 'quit having double': 357672,\n",
              " 'having double standard': 187948,\n",
              " 'double standard and': 128928,\n",
              " 'standard and do': 400951,\n",
              " 'and do the': 26962,\n",
              " 'do the same': 124626,\n",
              " 'the same to': 445064,\n",
              " 'same to the': 377201,\n",
              " 'to the murdeяξr': 481532,\n",
              " 'the murdeяξr in': 439029,\n",
              " 'murdeяξr in this': 281770,\n",
              " 'in this case': 215015,\n",
              " 'this case perlow': 462678,\n",
              " 'case perlow you': 88562,\n",
              " 'perlow you dissapoimtment': 339030,\n",
              " 'you dissapoimtment of': 541694,\n",
              " 'dissapoimtment of especially': 123005,\n",
              " 'of especially since': 302655,\n",
              " 'especially since officers': 138923,\n",
              " 'since officers seem': 390663,\n",
              " 'officers seem to': 312523,\n",
              " 'seem to be': 381993,\n",
              " 'to be getting': 470890,\n",
              " 'be getting the': 60499,\n",
              " 'getting the in': 170351,\n",
              " 'the in training': 435388,\n",
              " 'in training refreshers': 215275,\n",
              " 'training refreshers and': 486330,\n",
              " 'refreshers and taught': 364603,\n",
              " 'and taught to': 34705,\n",
              " 'taught to be': 413123,\n",
              " 'to be more': 471034,\n",
              " 'be more violent': 61237,\n",
              " 'more violent and': 278449,\n",
              " 'violent and agressive': 502485,\n",
              " 'and agressive cold': 24540,\n",
              " 'agressive cold and': 14461,\n",
              " 'cold and unhesitant': 97559,\n",
              " 'and unhesitant to': 36574,\n",
              " 'unhesitant to murder': 493908,\n",
              " 'to murder someone': 477318,\n",
              " 'murder someone on': 281705,\n",
              " 'someone on the': 396954,\n",
              " 'on the spot': 316764,\n",
              " 'the spot no': 446154,\n",
              " 'spot no questions': 400230,\n",
              " 'no questions asked': 290829,\n",
              " 'questions asked what': 357466,\n",
              " 'asked what joke': 51832,\n",
              " 'what joke lane': 516508,\n",
              " 'joke lane county': 239651,\n",
              " 'county district attorneys': 108580,\n",
              " 'district attorneys are': 123195,\n",
              " 'attorneys are nojusticeitisjustus': 55077,\n",
              " 'the usual total': 448746,\n",
              " 'usual total incompetence': 499167,\n",
              " 'total incompetence and': 485176,\n",
              " 'incompetence and light': 216688,\n",
              " 'and light sentences': 30461,\n",
              " 'light sentences for': 254006,\n",
              " 'sentences for murder': 383728,\n",
              " 'for murder in': 157485,\n",
              " 'murder in da': 281683,\n",
              " 'in da nei': 210306,\n",
              " 'productive members of': 352130,\n",
              " 'members of society': 271189,\n",
              " 'of society are': 307426,\n",
              " 'society are willing': 395123,\n",
              " 'are willing to': 46510,\n",
              " 'willing to pay': 526811,\n",
              " 'to pay up': 478017,\n",
              " 'pay up to': 335572,\n",
              " 'up to keep': 496344,\n",
              " 'to keep the': 476193,\n",
              " 'keep the civilization': 242708,\n",
              " 'the civilization functioning': 428851,\n",
              " 'civilization functioning that': 95106,\n",
              " 'functioning that provided': 166038,\n",
              " 'that provided the': 422111,\n",
              " 'provided the infrastructure': 354657,\n",
              " 'the infrastructure that': 435571,\n",
              " 'infrastructure that enabled': 218543,\n",
              " 'that enabled them': 419134,\n",
              " 'enabled them to': 136153,\n",
              " 'them to get': 453538,\n",
              " 'to get enough': 474481,\n",
              " 'get enough education': 169001,\n",
              " 'enough education to': 137369,\n",
              " 'education to earn': 133391,\n",
              " 'to earn good': 473458,\n",
              " 'earn good living': 131988,\n",
              " 'good living or': 173871,\n",
              " 'living or inherit': 257719,\n",
              " 'or inherit from': 322811,\n",
              " 'inherit from someone': 218611,\n",
              " 'from someone who': 164612,\n",
              " 'someone who did': 397053,\n",
              " 'who did taking': 521878,\n",
              " 'did taking the': 120061,\n",
              " 'taking the permanent': 412350,\n",
              " 'the permanent fund': 441451,\n",
              " 'permanent fund away': 339046,\n",
              " 'fund away from': 166060,\n",
              " 'away from everyone': 56055,\n",
              " 'from everyone would': 163814,\n",
              " 'everyone would kill': 142200,\n",
              " 'would kill our': 536107,\n",
              " 'kill our alaska': 243646,\n",
              " 'our alaska economy': 326841,\n",
              " 'alaska economy poor': 15195,\n",
              " 'economy poor people': 132948,\n",
              " 'poor people spend': 345481,\n",
              " 'people spend it': 337668,\n",
              " 'spend it all': 399477,\n",
              " 'it all here': 232194,\n",
              " 'all here from': 16514,\n",
              " 'here from my': 193694,\n",
              " 'from my may': 164218,\n",
              " 'my may 23rd': 283140,\n",
              " 'may 23rd letter': 267675,\n",
              " '23rd letter studies': 2887,\n",
              " 'letter studies by': 252064,\n",
              " 'studies by iser': 406265,\n",
              " 'by iser and': 80895,\n",
              " 'iser and the': 230922,\n",
              " 'and the institute': 35217,\n",
              " 'the institute on': 435636,\n",
              " 'institute on taxation': 219508,\n",
              " 'on taxation and': 316136,\n",
              " 'taxation and economic': 413708,\n",
              " 'and economic policy': 27204,\n",
              " 'economic policy show': 132718,\n",
              " 'policy show that': 344395,\n",
              " 'show that an': 388718,\n",
              " 'that an income': 417862,\n",
              " 'an income tax': 23344,\n",
              " 'income tax would': 216623,\n",
              " 'tax would hurt': 413697,\n",
              " 'would hurt alaska': 536060,\n",
              " 'hurt alaska families': 203877,\n",
              " 'alaska families the': 15200,\n",
              " 'families the least': 147636,\n",
              " 'the least in': 436770,\n",
              " 'least in an': 249562,\n",
              " 'in an email': 209323,\n",
              " 'an email my': 22865,\n",
              " 'email my senator': 135371,\n",
              " 'my senator claimed': 283520,\n",
              " 'senator claimed she': 383157,\n",
              " 'claimed she was': 95265,\n",
              " 'she was concerned': 386463,\n",
              " 'was concerned about': 506606,\n",
              " 'concerned about how': 101983,\n",
              " 'about how an': 6007,\n",
              " 'how an income': 201443,\n",
              " 'tax would affect': 413694,\n",
              " 'would affect small': 535252,\n",
              " 'affect small business': 11623,\n",
              " 'small business owners': 392360,\n",
              " 'business owners but': 77119,\n",
              " 'owners but who': 331664,\n",
              " 'but who do': 79631,\n",
              " 'who do you': 521926,\n",
              " 'do you think': 125073,\n",
              " 'you think are': 544438,\n",
              " 'think are the': 461206,\n",
              " 'are the customers': 45919,\n",
              " 'the customers for': 430273,\n",
              " 'customers for small': 112121,\n",
              " 'for small business': 158456,\n",
              " 'small business alaska': 392352,\n",
              " 'business alaska families': 76942,\n",
              " 'maybe one of': 268214,\n",
              " 'one of your': 318537,\n",
              " 'of your relatives': 311497,\n",
              " 'your relatives has': 547025,\n",
              " 'relatives has been': 365735,\n",
              " 'has been the': 182509,\n",
              " 'been the recipient': 65245,\n",
              " 'the recipient of': 443705,\n",
              " 'recipient of such': 363611,\n",
              " 'of such behavior': 307718,\n",
              " 'such behavior that': 407434,\n",
              " 'behavior that generally': 66169,\n",
              " 'that generally makes': 419491,\n",
              " 'generally makes the': 168332,\n",
              " 'makes the clueless': 263946,\n",
              " 'the clueless speak': 428980,\n",
              " 'clueless speak and': 97027,\n",
              " 'speak and act': 398715,\n",
              " 'and act more': 24396,\n",
              " 'act more compassionately': 9039,\n",
              " 'more compassionately toward': 277125,\n",
              " 'compassionately toward sexual': 101019,\n",
              " 'toward sexual assault': 485518,\n",
              " 'sexual assault victims': 385254,\n",
              " 'assault victims and': 52094,\n",
              " 'victims and since': 501797,\n",
              " 'and since you': 34038,\n",
              " 'since you asked': 390867,\n",
              " 'you asked the': 540961,\n",
              " 'asked the reason': 51820,\n",
              " 'the reason trump': 443631,\n",
              " 'reason trump is': 362936,\n",
              " 'trump is the': 489341,\n",
              " 'is the inspiration': 229527,\n",
              " 'the inspiration for': 435629,\n",
              " 'inspiration for the': 219186,\n",
              " 'for the women': 159504,\n",
              " 'the women march': 449789,\n",
              " 'women march is': 532201,\n",
              " 'march is because': 266157,\n",
              " 'is because he': 223555,\n",
              " 'because he is': 63328,\n",
              " 'he is in': 189422,\n",
              " 'is in position': 225694,\n",
              " 'in position to': 212855,\n",
              " 'position to also': 346237,\n",
              " 'to also assault': 470103,\n",
              " 'also assault women': 19281,\n",
              " 'assault women rights': 52100,\n",
              " 'women rights as': 532249,\n",
              " 'rights as guaranteed': 372439,\n",
              " 'as guaranteed by': 49312,\n",
              " 'guaranteed by the': 178055,\n",
              " 'by the constitution': 81751,\n",
              " 'the constitution obama': 429515,\n",
              " 'constitution obama was': 104019,\n",
              " 'obama was in': 299151,\n",
              " 'was in position': 507210,\n",
              " 'position to protect': 346246,\n",
              " 'to protect it': 478501,\n",
              " 'protect it but': 353972,\n",
              " 'it but the': 232807,\n",
              " 'but the gop': 79121,\n",
              " 'the gop wouldn': 433947,\n",
              " 'gop wouldn even': 174512,\n",
              " 'wouldn even consider': 536777,\n",
              " 'even consider his': 140137,\n",
              " 'consider his nomination': 103540,\n",
              " 'his nomination to': 197471,\n",
              " 'nomination to the': 291509,\n",
              " 'to the supreme': 481821,\n",
              " 'the supreme court': 446833,\n",
              " 'we re so': 512025,\n",
              " 're so limited': 360692,\n",
              " 'dynasty like korean': 131636,\n",
              " 'like korean kim': 254679,\n",
              " 'korean kim jong': 245793,\n",
              " 'mind your manners': 273973,\n",
              " 'your manners media': 546433,\n",
              " 'manners media congratulations': 265176,\n",
              " 'media congratulations congressman': 270346,\n",
              " 'congratulations congressman gianforte': 102814,\n",
              " 'not sure who': 295688,\n",
              " 'sure who told': 409825,\n",
              " 'who told you': 523047,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = vect_T.transform(X_train)\n",
        "print(T.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI-mga43sG6R",
        "outputId": "1c534122-15d5-48cd-fc4a-86dc84d74599"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el objeto de Regresión Linear\n",
        "regrT = LinearRegression()"
      ],
      "metadata": {
        "id": "ssiVPy95szC2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos nuestro modelo\n",
        "regrT.fit(T, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "2LUxvxMas40X",
        "outputId": "141370ef-a39c-4f04-c042-76ad68ada3d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos las predicciones que en definitiva una línea (en este caso, al ser 2D)\n",
        "yPredT = regrT.predict(T)"
      ],
      "metadata": {
        "id": "mWDf3kNRtD71"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos los coeficienetes obtenidos, En nuestro caso, serán la Tangente\n",
        "print('Coefficients: \\n', regrT.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoQ4DKLxtNBG",
        "outputId": "e7429969-bab6-4fb7-b346-e662a362eb5f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: \n",
            " [ 0.00256314  0.00198266 -0.00079259 ... -0.00096856 -0.0003136\n",
            " -0.0003136 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Este es el valor donde corta el eje Y (en X=0)\n",
        "print('Independent term: \\n', regrT.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv4ojQnytSVL",
        "outputId": "9070ee0a-6eee-4136-fe58-00042341ee30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Independent term: \n",
            " 0.041621920523004924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error Cuadrado Medio\n",
        "Trigramas = metrics.mean_squared_error(y_train, yPredT)\n",
        "print(\"Mean squared error:\" , Trigramas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS9VkGOCtTvv",
        "outputId": "0c9807f2-7aa4-4588-f483-a271fc56dae7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 0.00040916928643719723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbPv2INuM_gn"
      },
      "source": [
        "## Punto 3 - TfidfVectorizer\n",
        "\n",
        "Investigen sobre la función TfidfVectorizer. En la celda de texto 3, expliquen en qué consiste esta técnica de tokenización (describanla y expliquen su funcionamiento) y cúales son las ventajas o deventajas de su uso al compararlo con la función CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W34WMPKBM_gn"
      },
      "outputs": [],
      "source": [
        "#Celda 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBoMgxknM_go"
      },
      "source": [
        "## Punto 4 - Tokenización con TfidfVectorizer\n",
        "\n",
        "En la celda 4 creen y entrenen el mismo modelo de regresión del primer punto, para que prediga la probabilidad de que un comentario sea tóxico, usando los set de entrenamiento y test definidos anteriormente. Procesen los comentarios con la función **TfidfVectorizer** y presenten el desempeño del modelo con la métrica del MSE.\n",
        "\n",
        "Recuerden que el preprocesamiento que se haga sobre los datos de entrenamiento  (*.fit_transform()*) deben ser aplicado al set de test (*.transform()*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HGjxRDVDM_go"
      },
      "outputs": [],
      "source": [
        "# Celda 4\n",
        "# Creación de matrices de documentos usando CountVectorizer a partir de X\n",
        "vect_Tf = TfidfVectorizer()\n",
        "X_dtm_Tf = vect_Tf.fit_transform(X_train)\n",
        "temp_Tf=X_dtm_Tf.todense()\n",
        "#vect_Tf.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de diccionario de palabras con su respectivo ID asignado\n",
        "vect_Tf.vocabulary_"
      ],
      "metadata": {
        "id": "OqVpCPPJv8v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tf = vect_Tf.transform(X_train)\n",
        "print(Tf.toarray())"
      ],
      "metadata": {
        "id": "dWlpv8DEzWfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el objeto de Regresión Linear\n",
        "regrTf = LinearRegression()"
      ],
      "metadata": {
        "id": "zlabmpPgzhI8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos nuestro modelo\n",
        "regrTf.fit(Tf, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ae1puE4Uzk-_",
        "outputId": "6ece381f-fec8-4507-8b93-fc48a840f328"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos las predicciones que en definitiva una línea (en este caso, al ser 2D)\n",
        "yPredTf = regrTf.predict(Tf)"
      ],
      "metadata": {
        "id": "PJ3CdSxozz-Z"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error Cuadrado Medio\n",
        "TfidfVectorizer = metrics.mean_squared_error(y_train, yPredTf)\n",
        "print(\"Mean squared error:\" , TfidfVectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRXato-c0DB7",
        "outputId": "ecef3f22-acb1-45b8-b807-f28c464456d2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 7.161661403852852e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sp1Y9RPM_go"
      },
      "source": [
        "## Punto 5 - Comparación y análisis de resultados\n",
        "\n",
        "En la celda 5 comparen los resultados obtenidos de los diferentes modelos y comenten cómo el preprocesamiento de texto afecta el desempeño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqREHo7oM_go"
      },
      "outputs": [],
      "source": [
        "# Celda 5\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "name": "_merged",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}